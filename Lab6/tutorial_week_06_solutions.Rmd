---
title: "STATS 326/786"
author: "Week 6 Tutorial"
date: \today
fontsize: 11pt
output:
  bookdown::html_document2:
    fig_height: 5
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
    number_sections: false
    code_folding: show
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(tidyverse)
library(fpp3)
```


# Problem 1: Forecasting with decompositions

Below is a monthly time series of Auckland temperatures from July 1994 until January 2024.  

```{r, message = FALSE}
data = read_csv("auckland_temps.csv") %>%
  mutate(Month = yearmonth(Month)) %>%
  as_tsibble(index = Month)

data %>%
  autoplot(Temperature) +
  labs(y = "Temperature (\u00B0C)",
       title = "Monthly Average Temperatures in Auckland (Jul 1994 - Jan 2024)")
```

> Fit an STL decomposition to this time series setting `robust = TRUE`, then print the last 12 months from the decomposition table. 

```{r}
fit = data %>%
  model(stl = STL(Temperature, robust = TRUE)
  )
fit %>% 
  components() %>% 
  tail(n = 12) %>%
  select(Month, season_year, season_adjust)
```

> Using the printed `dable`, manually forecast $h = 1$ period ahead (to February 2024).  You should use the seasonal naive method for the seasonal component and the naive method for the seasonally-adjusted series.  Recall that you can add these two forecasts together to get your overall forecast.

```{r}
# Read off the table
s = 4.8  # The seasonal term from Feb 2023 (SNAIVE)
sa = 17.2	 # The seasonally-adjusted term from Jan 2024 (NAIVE)

# Overall forecast
s + sa
```

> Now write `R` code to fit an STL decomposition for forecasting, where the seasonal component uses the `SNAIVE` method (by default) and the seasonally-adjusted series uses the `NAIVE` method.  You will need to use the `decomposition_model` function.  

```{r}
fit = data %>%
  model(dcmp = decomposition_model(
    STL(Temperature, robust = TRUE),
    NAIVE(season_adjust)
  ))
```

> Overlay the fitted values on the time series and comment on the fit.  Comment on why there are missing values at the start of the series.  

```{r, warning = FALSE}
data %>% 
  autoplot(Temperature) + 
  autolayer(fit %>% 
              augment(),
            .fitted,
            col = 2) + 
  labs(y = "Temperature (\u00B0C)",
       title = "Monthly Average Temperatures in Auckland (Jul 1994 - Jan 2024)")
```

The fit looks reasonable.  There are missing values at the start because the seasonal naive method was used to fit the seasonal component.  The fitted values require the same month from the previous year, but since there is no data prior to July 1994, there will be 12 missing fitted values.  

> Perform a residual analysis to determine if you would trust this model for forecasting.  For the Ljung-Box test, choose `lag = 24` and `dof = 0`.  

```{r, warning = FALSE}

fit %>%
  gg_tsresiduals()

fit %>%
  augment() %>%
  features(.resid, 
           features = ljung_box, 
           lag = 24, 
           dof = 0)

```

The residuals are centered around 0 so point forecasts will not be biased.  They look reasonably normally distributed, but the variance has some mild heteroskedasticity (meaning the prediction intervals may be unreliable).  There is significant residual autocorrelation at lag 1, which contributes to a Ljung-Box $p$-value close to 0.  This suggests there is still information left in the residuals that would
need additional modelling.  The white noise assumptions are therefore not met and I would not trust this for forecasting in its current form.  

> Regardless of your conclusion to the previous question, forecast four years (48 months) ahead and plot.  Then compare the $h = 1$ forecast to your previously manually calculated one.  Are they the same?

```{r}
fc = fit %>%
  forecast(h = 48)

fc %>%
  autoplot(data) + 
  labs(y = "Temperature (\u00B0C)",
       title = "Monthly Average Temperatures in Auckland (Jul 1994 - Jan 2024)")

# Compare to hand calculated forecast
fc %>%
  head(n = 1)
```
Yes, the forecast is the same.


# Problem 2: Forecast accuracy

Consider the following time series.

```{r}
data <- tsibble(x = 1:10,
                y = c(1, 4, 5, 2, 3, 7, 6, 10, 8, 9),
                index = x)

data %>% autoplot(y) + xlab("x")
```

> Split this into a training set (the first six observations) and test set (the last four observations).  Name the training set `train` and the test set `test`.  

```{r}
train = data %>%
  filter(x %in% 1:6)
test = data %>%
  filter(x %in% 7:10)
```

> Once you have your training set, uncomment the code below.  This will plot the four period forecasts from the training set using the naive, average, and random-walk with drift methods.  Visually compare the forecasts with the true observations (test set).  Which method do you think will have the best forecast accuracy?  Which method do you think will have the worst forecast accuracy?  

```{r}
# Uncomment once you have your training set (called train)
fc = train %>%
  model(naive = NAIVE(y),
        average = MEAN(y),
        drift = RW(y ~ drift())) %>%
  forecast(h = 4)
fc %>% 
  autoplot(data, level = NULL) 
```

Best should be naive method as the forecasts are closer to the true values for three of the four time points.  Worst should be average method as forecasts are furthest away from the truth for all time points.  

> Verify your answer to the previous question by manually calculating the following forecast accuracy measures: MAE and RMSE.

```{r}

# Extract truth from test set
y = test %>% pull(y)

# Extract forecasts
fc.naive = fc %>% filter(.model == "naive") %>% pull(.mean)
fc.average = fc %>% filter(.model == "average") %>% pull(.mean) 
fc.drift =  fc %>% filter(.model == "drift") %>% pull(.mean)

# Calculate MAE
mean(abs(y - fc.naive))
mean(abs(y - fc.average))
mean(abs(y - fc.drift))

# Calculate RMSE
sqrt(mean((y - fc.naive) ^ 2))
sqrt(mean((y - fc.average) ^ 2))
sqrt(mean((y - fc.drift) ^ 2))

# A check using accuracy function
fc %>%
  accuracy(data) %>%
  select(.model, MAE, RMSE)

# Alternative, "tidier" way

# Join forecast and test set
fc = fc %>% 
  as_tibble() %>%  # Easier to manipulate tibble
  select(-y)       # Remove y variable
fc.join = left_join(fc, test, by = "x") 
fc.join

# Compute accuracy measures for each model
fc.join %>%
  group_by(.model) %>%
  summarise(MAE = mean(abs(y - .mean)),
            RMSE = sqrt(mean((y - .mean) ^ 2)))
```

Just an aside to keep in mind.  Note that forecast errors are different from residuals in two ways. First, residuals are calculated on the training set while forecast errors are calculated on the test set. Second, residuals are based on one-step forecasts while forecast errors can involve multi-step forecasts.


